# RMIT Policy Assistant with Manual RAG using Bedrock and FAISS
# app.py

import streamlit as st # Ensure this is the first st command if not for set_page_config
import json
import boto3
import os
import sqlite3
import numpy as np
import re
import faiss # For local vector search
import uuid # For session IDs
from sentence_transformers import SentenceTransformer # For local embeddings

# PyPDF2 is kept as requested by the user, though not used in the primary RAG workflow.
from PyPDF2 import PdfReader 

#define DB path
DB_PATH = "chat_sessions.db"

# --- SESSION HELPER FUNCTIONS ---
def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    # Sessions table
    c.execute("""
        CREATE TABLE IF NOT EXISTS session_metadata (
            session_id TEXT PRIMARY KEY,
            session_name TEXT UNIQUE,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    """)
    # Messages table
    c.execute("""
        CREATE TABLE IF NOT EXISTS chat_messages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            session_id TEXT,
            role TEXT,
            content TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (session_id) REFERENCES session_metadata(session_id)
        )
    """)
    conn.commit()
    conn.close()

def create_new_session(session_name):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    session_id = str(uuid.uuid4())
    try:
        c.execute("INSERT INTO session_metadata (session_id, session_name) VALUES (?, ?)", (session_id, session_name))
        conn.commit()
        return session_id
    except sqlite3.IntegrityError:
        return None
    finally:
        conn.close()

def get_existing_sessions():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT session_name FROM session_metadata ORDER BY created_at DESC")
    sessions = [row[0] for row in c.fetchall()]
    conn.close()
    return sessions

def get_session_id_by_name(session_name):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT session_id FROM session_metadata WHERE session_name = ?", (session_name,))
    row = c.fetchone()
    conn.close()
    return row[0] if row else None

def load_chat_history(session_id):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT role, content FROM chat_messages WHERE session_id = ? ORDER BY timestamp", (session_id,))
    rows = c.fetchall()
    conn.close()
    return [{"role": row[0], "content": row[1]} for row in rows]

def log_chat_message(session_id, role, content):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("INSERT INTO chat_messages (session_id, role, content) VALUES (?, ?, ?)", (session_id, role, content))
    conn.commit()
    conn.close()

# Initialize DB tables on app startup
init_db()

# === AWS Configuration === #
REGION = "us-east-1"
# LLM for generating answers
LLM_MODEL_ID = "anthropic.claude-3-haiku-20240307-v1:0"
# Embeddings model for text vectorization (used by build_index.py, name referenced here)
LOCAL_EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2' 

# Replace with your Cognito User Pool credentials
IDENTITY_POOL_ID = "us-east-1:7771aae7-be2c-4496-a582-615af64292cf" # Replace if different
USER_POOL_ID = "us-east-1_koPKi1lPU" # Replace if different
APP_CLIENT_ID = "3h7m15971bnfah362dldub1u2p" # Replace if different
USERNAME = "ch530112518@gmail.com" # Replace with your username
PASSWORD = "Aaron012#"    # Replace with your password

# === Paths for Pre-built Index and Chunks (Generated by Offline Script) === #
FAISS_INDEX_PATH = "rmit_policies_local.faiss_index"
CHUNKS_DATA_PATH = "rmit_policy_chunks_local.json"

# --- Streamlit Page Configuration (MUST BE THE FIRST STREAMLIT COMMAND) ---
st.set_page_config(page_title="RMIT Policy Assistant", layout="wide")

if "session_id" not in st.session_state:
    st.session_state.session_id = None  # No session selected yet

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# === Helper Functions === #
def get_credentials(username, password):
    """Authenticates with Cognito and retrieves temporary AWS credentials."""
    try:
        idp_client = boto3.client("cognito-idp", region_name=REGION)
        response = idp_client.initiate_auth(
            AuthFlow="USER_PASSWORD_AUTH",
            AuthParameters={"USERNAME": username, "PASSWORD": password},
            ClientId=APP_CLIENT_ID,
        )
        id_token = response["AuthenticationResult"]["IdToken"]

        identity_client = boto3.client("cognito-identity", region_name=REGION)
        identity_response = identity_client.get_id(
            IdentityPoolId=IDENTITY_POOL_ID,
            Logins={f"cognito-idp.{REGION}.amazonaws.com/{USER_POOL_ID}": id_token},
        )

        creds_response = identity_client.get_credentials_for_identity(
            IdentityId=identity_response["IdentityId"],
            Logins={f"cognito-idp.{REGION}.amazonaws.com/{USER_POOL_ID}": id_token},
        )
        return creds_response["Credentials"]
    except Exception as e:
        # Changed from st.error to print for functions that might be called before UI is fully ready
        # or if this function were to be used by build_index.py (though it's not currently).
        # UI errors will be handled where this function is called if needed.
        print(f"Authentication Error: Could not get AWS credentials. {e}")
        return None

def invoke_llm_on_bedrock(prompt_text, model_id=LLM_MODEL_ID, max_tokens=2048, temperature=0.2, top_p=0.9):
    """Invokes the Bedrock LLM with the given prompt and returns the response."""
    credentials = get_credentials(USERNAME, PASSWORD)
    if not credentials:
        st.error("LLM Invocation Error: Authentication failed.") # This st.error is fine as it's called after UI setup
        return "Error: Authentication failed. Cannot contact the AI assistant."

    bedrock_runtime = boto3.client(
        "bedrock-runtime",
        region_name=REGION,
        aws_access_key_id=credentials["AccessKeyId"],
        aws_secret_access_key=credentials["SecretKey"],
        aws_session_token=credentials["SessionToken"],
    )

    payload = {
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": top_p,
        "messages": [{"role": "user", "content": [{"type": "text", "text": prompt_text}]}]
    }

    try:
        response = bedrock_runtime.invoke_model(
            body=json.dumps(payload),
            modelId=model_id,
            contentType="application/json",
            accept="application/json"
        )
        result = json.loads(response["body"].read())
        
        if "content" not in result or not result["content"] or "text" not in result["content"][0]:
            st.error(f"LLM Error: Received an unexpected response format from the model: {result}")
            return "Error: The AI assistant provided an unexpected response."
        
        return result["content"][0]["text"]
    except Exception as e:
        st.error(f"LLM Invocation Error: {e}")
        return "Error: Could not get a response from the AI assistant."

def validate_user_question(question_text):
    banned_words = ["hate", "terrorism", "kill", "bomb"]  # Extend as needed
    lower_q = question_text.lower()
    if not question_text.strip():
        return False, "Your question is empty. Please ask something."
    if any(word in lower_q for word in banned_words):
        return False, "Your question contains inappropriate content and cannot be processed."
    return True, ""

# === Streamlit Application Logic ===

@st.cache_resource # Cache the loaded index and chunks
def load_local_rag_components():
    """Loads the FAISS index, policy text chunks, and local embedding model."""
    print("Attempting to load local RAG components...")
    
    # Load Sentence Transformer Model
    loaded_embedding_model = None
    try:
        loaded_embedding_model = SentenceTransformer(LOCAL_EMBEDDING_MODEL_NAME)
        print(f"Local embedding model '{LOCAL_EMBEDDING_MODEL_NAME}' loaded successfully.")
    except Exception as e:
        # Use print here as st.error might not be ideal if called too early by other parts
        # The main app will show an error if this model is None
        print(f"Fatal Error during RAG component loading: Could not load local sentence-transformer model '{LOCAL_EMBEDDING_MODEL_NAME}'. {e}")
        # No st.error here, will be handled by the calling code checking for None

    # Load FAISS Index and Chunks
    loaded_faiss_index = None
    loaded_policy_chunks = []
    if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(CHUNKS_DATA_PATH):
        try:
            loaded_faiss_index = faiss.read_index(FAISS_INDEX_PATH)
            with open(CHUNKS_DATA_PATH, "r", encoding="utf-8") as f:
                loaded_policy_chunks = json.load(f)
            print(f"Successfully loaded FAISS index with {loaded_faiss_index.ntotal if loaded_faiss_index else 'N/A'} vectors and {len(loaded_policy_chunks)} text chunks.")
        except Exception as e:
            print(f"Error loading FAISS index or chunks from disk: {e}")
            # No st.error here, will be handled by the calling code
    else:
        print(f"Knowledge base files not found: {FAISS_INDEX_PATH} or {CHUNKS_DATA_PATH}. Indexing may be needed.")
        # No st.warning here, will be handled by the calling code

    return loaded_embedding_model, loaded_faiss_index, loaded_policy_chunks

# --- Load Knowledge Base and Embedding Model (AFTER st.set_page_config) ---
local_embedding_model, faiss_index_global, policy_chunks_data_global = load_local_rag_components()

def get_local_embedding(text_chunk):
    """Generates an embedding for a text chunk using the loaded local model."""
    if local_embedding_model is None:
        print("Error: Local embedding model not loaded. Cannot generate embedding.")
        return None
    try:
        # model.encode() returns a numpy array or list of arrays
        embedding = local_embedding_model.encode(text_chunk)
        return embedding 
    except Exception as e:
        print(f"Error generating local embedding: {e}")
        return None

def retrieve_relevant_chunks_locally(user_question_embedding_np, k=3):
    """Retrieves top_k relevant chunks from FAISS index."""
    if faiss_index_global is None or not policy_chunks_data_global:
        # This message will appear in the UI if the button is pressed and KB isn't loaded
        st.warning("Knowledge base not loaded. Cannot retrieve policy sections.")
        return [], [] 

    if user_question_embedding_np is None:
        st.error("Could not generate embedding for the question. Cannot retrieve policy sections.")
        return [], []
    
    num_indexed_items = faiss_index_global.ntotal
    actual_k = min(k, num_indexed_items)
    
    if actual_k == 0: # No items in index
        st.info("The knowledge base is currently empty.")
        return [], []

    try:
        # FAISS search expects a 2D numpy array (n_queries, dimension)
        distances, indices = faiss_index_global.search(user_question_embedding_np, actual_k)
        
        retrieved_texts = []
        retrieved_sources = []
        for i in indices[0]: # indices[0] because search input was a single query vector
            if 0 <= i < len(policy_chunks_data_global): # Check bounds
                chunk_data = policy_chunks_data_global[i]
                retrieved_texts.append(chunk_data.get("text", ""))
                retrieved_sources.append(chunk_data.get("source", "Unknown source"))
            else:
                print(f"Warning: FAISS index {i} out of bounds for policy_chunks_data_global (len: {len(policy_chunks_data_global)}).")
        
        unique_sources = sorted(list(set(retrieved_sources)))
        return retrieved_texts, unique_sources
    except Exception as e:
        st.error(f"Error during FAISS search: {e}")
        return [], []

# --- Streamlit UI Elements (AFTER st.set_page_config and data loading) ---
st.title("ðŸ›ï¸ RMIT Policy Assistant")
st.markdown(
    "Ask questions about RMIT policies. This assistant uses a local knowledge base of policy documents to find relevant information."
)

# Display loading status in the sidebar (AFTER set_page_config)
if local_embedding_model is None:
    st.sidebar.error("Local Embedding Model FAILED to load. Check console.")
else:
    st.sidebar.success("Local Embedding Model Loaded.")

if faiss_index_global is None or not policy_chunks_data_global:
    st.sidebar.error("Knowledge Base (FAISS/Chunks) NOT Loaded. Run build_index.py.")
else:
    st.sidebar.success(f"Knowledge Base Loaded ({faiss_index_global.ntotal} policy chunks)")

# --- SIDEBAR ---
st.sidebar.header("Session Management")

sessions = get_existing_sessions()

selected_session_name = st.sidebar.selectbox("Select a session", ["New Session"] + sessions)

if selected_session_name == "New Session":
    new_name = st.sidebar.text_input("Enter new session name")
    if st.sidebar.button("Create Session"):
        if not new_name.strip():
            st.sidebar.error("Please enter a session name.")
        elif new_name in sessions:
            st.sidebar.error("Session name already exists.")
        else:
            session_id = create_new_session(new_name)
            if session_id:
                st.session_state.session_id = session_id
                st.session_state.chat_history = []
                st.sidebar.success(f"Created and switched to session '{new_name}'.")
                st.rerun()
            else:
                st.sidebar.error("Failed to create session.")
else:
    # Load chat if session changed
    session_id = get_session_id_by_name(selected_session_name)
    if session_id and st.session_state.get("session_id") != session_id:
        st.session_state.session_id = session_id
        st.session_state.chat_history = load_chat_history(session_id)
        st.sidebar.success(f"Loaded session '{selected_session_name}' with {len(st.session_state.chat_history)} messages.")
        st.rerun()

# Button to clear chat history for current session
if st.sidebar.button("Clear Current Session Chat History"):
    if st.session_state.session_id:
        conn = sqlite3.connect(DB_PATH)
        c = conn.cursor()
        c.execute("DELETE FROM chat_messages WHERE session_id = ?", (st.session_state.session_id,))
        conn.commit()
        conn.close()
        st.session_state.chat_history = []
        st.sidebar.success("Cleared chat history for current session.")
        st.rerun()


# --- Initialize session state for chat history and session ID ---
if "session_id" not in st.session_state:
    st.session_state["session_id"] = None
    st.session_state["session_name"] = None
    st.session_state["chat_history"] = []

# --- Display Chat History ---
st.subheader("Conversation")
# Use a container with a fixed height and scroll for chat history
chat_display_container = st.container(height=400) 
with chat_display_container:
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# --- User Question Input ---
user_question = st.chat_input("What would you like to know about RMIT policies?")

# --- RESPONSIBLE AI RULES ---
def validate_user_question(question_text):
    # Basic example: block if question is empty or contains banned words
    banned_words = ["hate", "terrorism", "kill", "bomb"]  # expand as needed
    lower_q = question_text.lower()
    if not question_text.strip():
        return False, "Your question is empty. Please ask something."
    if any(word in lower_q for word in banned_words):
        return False, "Your question contains inappropriate content and cannot be processed."
    # Add other rules as needed
    return True, ""

def log_responsible_ai_issue(issue_description):
    # Simple console logging; extend this to DB or monitoring as needed
    print(f"[Responsible AI Alert] {issue_description}")

def filter_ai_response(response_text):
    """
    Filters AI response text to remove or redact disallowed content,
    appends disclaimers if necessary, and logs incidents.
    """
    if not response_text:
        return response_text  # Nothing to filter

    banned_words = [
        "kill", "bomb", "terrorism", "hate speech", "illegal", "discriminate", "violence"
    ]

    lower_text = response_text.lower()
    filtered = False

    # Redact banned words
    for word in banned_words:
        if word in lower_text:
            # Replace all case variations with '[redacted]'
            # Simple case insensitive replace: iterate to find all occurrences
            import re
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            response_text = pattern.sub("[redacted]", response_text)
            filtered = True

    if filtered:
        log_responsible_ai_issue("Sensitive content detected and redacted in AI response.")
        response_text += "\n\n[Note: Potentially sensitive content was removed.]"

    # Detect phrases that indicate refusal or uncertainty
    refusal_phrases = [
        "i am not sure", "i do not have information", "cannot answer",
        "insufficient information", "i'm sorry", "don't know"
    ]
    if any(phrase in lower_text for phrase in refusal_phrases):
        response_text += (
            "\n\n[Disclaimer: The assistant is limited to available policy information "
            "and cannot provide answers beyond that scope.]"
        )

    return response_text

if user_question:
    valid, msg = validate_user_question(user_question)
    if not valid:
        st.session_state.chat_history.append({"role": "assistant", "content": msg})
        st.rerun()

if user_question:
    st.session_state.chat_history.append({"role": "user", "content": user_question})
    if st.session_state.session_id:
        log_chat_message(st.session_state.session_id, "user", user_question)
    else:
        st.warning("No session selected. Please create or select a session.")
    st.rerun()

# --- Process the latest user question if it exists ---
# This logic runs after a potential rerun from input, so history is up-to-date
if st.session_state.chat_history and st.session_state.chat_history[-1]["role"] == "user":
    # Check if the last message was from the user and an AI response hasn't been generated for it yet.
    # This simple check might need refinement for more complex scenarios (e.g., if AI fails and user asks again).
    # A more robust way is to have a flag like "needs_processing" or check if last message is user and second last is not assistant for that user message.
    # For now, let's assume if last is user, we process.

    last_user_question = st.session_state.chat_history[-1]["content"]

    if not (faiss_index_global and policy_chunks_data_global and local_embedding_model):
        # Error/Warning messages about unloaded components are already in the sidebar.
        # Add a message to the chat if user tries to interact.
        error_message = "Cannot process your question because essential RAG components (embedding model or knowledge base) are not loaded. Please check setup and console logs."
        st.session_state.chat_history.append({"role": "assistant", "content": ai_response_text})

        st.session_state.chat_history.append({"role": "assistant", "content": ai_response_text})

        if st.session_state.session_id:
            log_chat_message(st.session_state.session_id, "assistant", ai_response_text)

        st.rerun()

    else:
        ai_response_text = "" # Initialize
        try:
            # --- RAG Pipeline ---
            with st.spinner("Thinking... Generating embedding for your question..."):
                # Ensure get_local_embedding returns a NumPy array suitable for FAISS
                question_embedding_single = get_local_embedding(last_user_question) # This is a 1D array
                if question_embedding_single is not None:
                    # FAISS search expects a 2D array (n_queries, dimension)
                    question_embedding_np = np.array([question_embedding_single]).astype('float32')
                else:
                    question_embedding_np = None
            
            if question_embedding_np is not None:
                with st.spinner("Searching relevant policy sections..."):
                    retrieved_texts, retrieved_sources = retrieve_relevant_chunks_locally(question_embedding_np, k=3) 
                
                context_for_prompt = "No specific policy sections were found for this question in the knowledge base."
                if retrieved_texts:
                    context_for_prompt = "\n\n---\n\n".join(retrieved_texts)
                
                # --- Construct Prompt with History and Context ---
                system_policy_prompt = (
                    "You are an AI assistant from RMIT University, an expert in RMIT policies. "
                    "Follow these Responsible AI principles strictly:\n"
                    "1. Only answer based on the provided policy context. Do NOT guess or invent information.\n"
                    "2. If the retrieved policy sections do not contain sufficient information, respond honestly that the information is insufficient.\n"
                    "3. Never provide personal opinions, medical, legal, or financial advice.\n"
                    "4. Maintain a respectful, neutral, and professional tone at all times.\n"
                    "5. Do not generate or endorse harmful, biased, or inappropriate content.\n"
                    "6. If a question is outside the scope of RMIT policies or your knowledge, politely state that you cannot answer.\n"
                    "7. Be transparent about limitations.\n\n"
                    "Your sole purpose is to help students and staff understand official RMIT policies based on the provided context.\n"
                    "Explain policies clearly and simply.\n"
                    "Consider the previous conversation turns for context."
                )

                history_for_prompt = ""
                recent_history = st.session_state.chat_history[-5:-1]  # last 4 messages excluding current question
                for turn in recent_history:
                    history_for_prompt += f"{turn['role'].title()}: {turn['content']}\n"

                prompt_for_bedrock = f"{system_policy_prompt}\n\n"
                if history_for_prompt:
                    prompt_for_bedrock += (
                        "Previous conversation turns for context (ignore your own previous responses if they seem like part of this current thought process):\n"
                        f"{history_for_prompt}\n"
                    )

                prompt_for_bedrock += (
                    "Relevant sections from RMIT Policy Documents (use ONLY these for your answer):\n"
                    "<retrieved_policy_sections>\n"
                    f"{context_for_prompt}\n"
                    "</retrieved_policy_sections>\n\n"
                    f"Based strictly on the conversation history and the retrieved policy sections above, answer this Current User Question: {last_user_question}\n\n"
                    "Note: If the answer is based on incomplete or insufficient information, clearly state this. "
                    "Do not provide advice beyond RMIT policies. "
                    "Be transparent about your limitations and avoid speculation."
                )

                # For debugging:
                # with st.expander("View Full Prompt Sent to Bedrock"):
                #     st.text(prompt_for_bedrock)

                # --- Invoke LLM ---
                with st.spinner("Consulting RMIT policies and generating response..."):
                    raw_ai_response = invoke_llm_on_bedrock(prompt_for_bedrock)
                    ai_response_text = filter_ai_response(raw_ai_response)

                
                if not ai_response_text: # Check if response is None or empty
                    ai_response_text = "Sorry, I encountered an issue and could not generate a response."
                
                

                # Add AI response to chat history
                st.session_state.chat_history.append({"role": "assistant", "content": ai_response_text})



                if st.session_state.session_id:
                    log_chat_message(st.session_state.session_id, "assistant", ai_response_text)

                
                # --- Placeholder for SQLite Logging ---
                # log_to_sqlite(st.session_state.session_id, "user", last_user_question)
                # log_to_sqlite(st.session_state.session_id, "assistant", ai_response_text)
                # print(f"Logged to SQLite (conceptual): User: {last_user_question}, Assistant: {ai_response_text}")
                st.rerun() # Rerun to display the new AI message

            else: # if not question_embedding_np
                error_msg_embedding = "Could not process your question (failed to get local embedding). Please try again."
                st.session_state.chat_history.append({"role": "assistant", "content": error_msg_embedding})
                if st.session_state.session_id:
                    log_chat_message(st.session_state.session_id, "assistant", error_msg_embedding)
                st.rerun()

        except Exception as e:
            error_msg_exception = f"An unexpected error occurred: {str(e)}"
            st.session_state.chat_history.append({"role": "assistant", "content": error_msg_exception})
            if st.session_state.session_id:
                log_chat_message(st.session_state.session_id, "assistant", error_msg_exception)
            # To see the full error in Streamlit during development:
            # st.exception(e) 
            st.rerun()


# --- Sidebar Elements ---
st.sidebar.markdown("---")
if st.session_state.session_id:
    st.sidebar.write(f"**Current Session ID:** `{st.session_state.session_id}`")
else:
    st.sidebar.write("No session selected.")
